{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import some useful packages and set everything up (e.g., inline plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from math import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with defining the solution for least squares regression in closed form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute parameters of the linear regression of phi(x)->y\n",
    "# Inputs:\n",
    "#   x, y: the training data and labels, respectively\n",
    "#   transform: function mapping x to features phi(x)\n",
    "# Output:\n",
    "# w : parameters of the trained model\n",
    "def lr_closed_form(x,y,transform):\n",
    "    return np.dot(np.linalg.pinv(transform(x)),y)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below (that perhaps could be simplified) defines a generalized polynomial feature expansion, specified as a list of degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyFv(x,degs,mu=None,sigma=None):\n",
    "    # Map m-dim input vector x to [x_1^d_1, x_2^d_1,...,x_m^d_1,x_1^d_2,...,x_m^d_k]\n",
    "    # where degs is the list (d_1,...,d_k)\n",
    "    # if d_j=0, only include a single copy of 1 (and not m copies)\n",
    "    #\n",
    "    # If mu and sigma are passed, this function will normalize each feature j, subtracting mu[j] and dividing by sigma[j].\n",
    "    # If mu and sigma are not passed: they will be calculated, and returned as additional outputs, so the function will\n",
    "    #   return a tuple (X,mu,sigma); if you want to just compute the feature map, but do not pass mu/sigma, then use\n",
    "    #   polyFv(x)[0]\n",
    "    \n",
    "    nd=len(degs)\n",
    "    \n",
    "    if len(x.shape)==1:\n",
    "        x=x[:,np.newaxis] # force a \"data matrix\" form, even if there's only a single feature\n",
    "    N,m=x.shape\n",
    "    \n",
    "    # calculate the dimension of the transformed feature vector phi(x)\n",
    "    if 0 in degs:\n",
    "        M=(len(degs)-1)*m+1\n",
    "    else:\n",
    "        M=len(degs)*m\n",
    "        \n",
    "    X=np.empty([N,M])\n",
    "    if 0 in degs:\n",
    "        X[:,0]=1\n",
    "        j=1\n",
    "    else:\n",
    "        j=0 # index to keep track of where to write the next chunk of features\n",
    "        \n",
    "    for d in range(len(degs)):\n",
    "        if degs[d] != 0:\n",
    "            X[:,j:j+m]=x**degs[d]\n",
    "            j=j+m\n",
    "            \n",
    "    # normalize\n",
    "    if mu is None:\n",
    "        mu = X.mean(0)\n",
    "        sigma = X.std(0)\n",
    "        sigma[0]=1\n",
    "        sigma[sigma<0.0001]=1 # to avoid division by zero\n",
    "        return_normalizer = True\n",
    "    else:\n",
    "        return_normalizer = False\n",
    "        \n",
    "    X=(X-mu)/sigma\n",
    "    \n",
    "    if 0 in degs:\n",
    "        X[:,0]=1\n",
    "\n",
    "    if return_normalizer:\n",
    "        return X, mu, sigma\n",
    "    else:\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy experiment with 1D regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we work on a real dataset with a multi-dimensional input space, let's explore regression in 1D inputs, which is easy to visualize. We will do this with a <i>synthetic</i> data domain, where we simply generate the data from a noisy model.\n",
    "Note that in this case, contrary to the real world scenario, we can ask how well the regression method recovers the true model; in the real world case this is of course not a meaningful question because we will never know the true model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate range of 1d xs, from -2 to 2, with a given step\n",
    "def genXrange(step):\n",
    "    xt=np.arange(-2,2,step)\n",
    "    xtick=np.empty([xt.shape[0],1])\n",
    "    xtick[:,0]=xt\n",
    "    return xtick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a dataset {(x,y)} where y = f(x) + noise\n",
    "# n: # of examples\n",
    "# sigma: std. deviation of the noise in y\n",
    "# f: the function accounting for the deterministic component in y\n",
    "def genD(n,sigma,f):\n",
    "    \n",
    "    # first get xs -- jitter them around a bit with uniform noise\n",
    "    xraw=np.linspace(-2,2,n)\n",
    "    xnoise=np.random.uniform(-.1,.1,xraw.shape)\n",
    "    x=np.empty([xraw.shape[0],1])\n",
    "    x[:,0]=xraw+xnoise\n",
    "    \n",
    "    # now generate ys\n",
    "    y=f(x)\n",
    "    ynoise=np.random.normal(0,sigma,y.shape)\n",
    "    y=np.squeeze(y+ynoise)\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function we will be using as the source of our x->y data. You can see how modifying f,\n",
    "# for example making it more or less smooth, affects the experiments below.\n",
    "def f(x):\n",
    "    return np.sin(x*1.7)+.1*x**2+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's test it out: generate a dataset and plot it\n",
    "x, y = genD(20,.3,f)\n",
    "\n",
    "plt.plot(x,y,'r*')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');\n",
    "\n",
    "# and also plot the function over a dense 1D \"grid\" of x values\n",
    "xtick=genXrange(.01)\n",
    "plt.plot(xtick,f(xtick),'r:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to build a simple toolkit for 1D regression. It will be limited to solving least squares in closed form, but powerful enough to allow non-linear features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some feature maps\n",
    "# we could define a separate function for each set of features, but using lambda expressions\n",
    "# seems more convenient\n",
    "mapLinear = lambda x: polyFv(x,(0,1))[0]\n",
    "map2 = lambda x: polyFv(x,(0,1,2))[0]\n",
    "map3 = lambda x: polyFv(x,range(0,4))[0]\n",
    "map7 = lambda x: polyFv(x,range(0,8))[0]\n",
    "map15 = lambda x: polyFv(x,range(0,16))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to conveniently visualize the regression model, plotting it over a dense grid of inputs\n",
    "# Input:\n",
    "#   w : the regression model (parameter vector)\n",
    "#   transform: the feature transform mapping x to phi(x)\n",
    "#   style: line style\n",
    "#   step: density of the grid on which the model is visualized\n",
    "def plotLR(w,transform,style='k-',step=0.01):\n",
    "    # sample inputs (xs) on a dense grid with given step\n",
    "    xtick=genXrange(step)\n",
    "    # predict y for each x using the model\n",
    "    yhat=np.dot(transform(xtick),w)\n",
    "    # draw the line (note: plot will interpolate the predictions, making it appear as a continuous line)\n",
    "    plt.plot(xtick,yhat,style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=lr_closed_form(x,y,mapLinear)\n",
    "w2=lr_closed_form(x,y,map2)\n",
    "w7=lr_closed_form(x,y,map7)\n",
    "w15=lr_closed_form(x,y,map15)\n",
    "\n",
    "plt.plot(x,y,'r*')\n",
    "xtick=genXrange(.01)\n",
    "plt.plot(xtick,f(xtick),'r:')\n",
    "\n",
    "plotLR(w1,mapLinear,'k-')\n",
    "plotLR(w2,map2,'g-')\n",
    "plotLR(w7,map7,'b-')\n",
    "\n",
    "plotLR(w15,map15,'c')\n",
    "plt.ylim(-1,5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as in the examples we saw in class, this shows that more complex models can fit the <i>training</i> data better. \n",
    "We can evaluate these models numerically, by computing training loss (avg. squared error) as well as the test loss (on the test set we will shortly generate). We will define an actual Python function that implements our (mathematical) loss function. \n",
    "\n",
    "Ignore the gradient related bits for now; we will need them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossSquared(X,y,w,getGrad=False):\n",
    "    yhat = np.dot(X,w)\n",
    "    err = np.squeeze(yhat)-y # prediction error values, for all the points in x\n",
    "    loss = np.mean(err**2)\n",
    "    if getGrad:\n",
    "        grad = 2*np.mean(X*err.reshape(err.shape[0],1),axis=0)\n",
    "        return loss, grad.reshape(w.shape)\n",
    "    else:\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to use the same noise parameters and f as in generating the training set!\n",
    "xTest, yTest = genD(100,.3,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ms = (0,1,2,3,5,7,15,25)\n",
    "polyModels = {}\n",
    "errTrain = np.zeros((len(Ms)))\n",
    "\n",
    "errTest = np.zeros((len(Ms)))\n",
    "\n",
    "for m in range(len(Ms)):\n",
    "    _,mu,sigma=polyFv(x,range(0,Ms[m]+1))\n",
    "    transform = lambda x: polyFv(x,range(0,Ms[m]+1),mu,sigma)\n",
    "    polyModels[m] = lr_closed_form(x,y,transform)\n",
    "    errTrain[m] = lossSquared(transform(x),y,polyModels[m])\n",
    "    errTest[m] = lossSquared(transform(xTest),yTest,polyModels[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t\\tTrain err\\tTest err\\n')\n",
    "for m in range(len(Ms)):\n",
    "    print('model %03d:\\t%.04f\\t\\t%.04f'%(Ms[m],errTrain[m],errTest[m]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as expected, more complex models reduce training RMSE, but at some point along the complexity axis, the test RMSE starts increasing, due to overfitting (this is also graphically evident in the plot, of course).\n",
    "\n",
    "It would be interesting to see how this table would change if we had more training data than 20 points..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Dataset: Boston Housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready for some more realistic experiments. Let's consider the Boston Housing data set, originally from the UCI data repository. It's packaged with the scikit-learn distribution available with Anaconda, so it's easy to load it. It's in a rather fancy format, containing in addition to the raw data a description explaining what the features are.\n",
    "\n",
    "The task here is to predict median house prices (in circa 1980 dollars, so don't be shocked! :-) based on various measurements of the neighborhoods in Boston."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_boston\n",
    "boston=load_boston()\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhouse=boston.data\n",
    "yhouse=boston.target\n",
    "# visualize house price as a function of, say, \"avg. number of rooms in a dwelling\"\n",
    "plt.plot(xhouse[:,5],yhouse,'r.')\n",
    "plt.xlabel('avg # rooms')\n",
    "plt.ylabel('house price');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first shuffle the data; to make sure everyone is working with the same partitions, we will fix the seed of the pseudorandom number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(54321)\n",
    "p = np.random.permutation(xhouse.shape[0])\n",
    "xhouse=xhouse[p,:]\n",
    "yhouse=yhouse[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to partition the data into train, val, and test. Let's put approximately 70% in train, 15% in val, and 15% in test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Ntrain=350\n",
    "Nval=75\n",
    "Ntest=xhouse.shape[0]-Ntrain-Nval\n",
    "\n",
    "xtrain = xhouse[0:Ntrain,:]\n",
    "ytrain = yhouse[0:Ntrain]\n",
    "xval = xhouse[Ntrain:Ntrain+Nval,:]\n",
    "yval = yhouse[Ntrain:Ntrain+Nval]\n",
    "xtest = xhouse[Ntrain+Nval:,:]\n",
    "ytest = yhouse[Ntrain+Nval:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the closed form solution, we will now try to learn the model using gradient descent.\n",
    "\n",
    "Let's define a general-ish gradient descent routine. It will take as input the data (x,y), and the specifications of how to map the x to the feature space; how to run optimization; and how to evaluate loss (and its gradient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD(x,y,transform,optim,lfunc):\n",
    "    X = transform(x)\n",
    "    w = np.zeros((X.shape[1]))\n",
    "    converged = False\n",
    "    \n",
    "    lr=optim['lr']\n",
    "    \n",
    "    loss=np.empty([optim['maxiter']]) # will keep track of training loss\n",
    "    \n",
    "    t = 0 # iteration count\n",
    "    while not converged:\n",
    "        loss[t],lgrad = lfunc(X,y,w,getGrad=True)\n",
    "        \n",
    "        #### YOUR CODE HERE for:\n",
    "        # 1) computing the gradient-based update;\n",
    "        # 2) checking for convergence;\n",
    "        # 3) updating learning rate and doing anything else you want to do as part of the optimization routine\n",
    "        # look at the keys of optim dictionary below for inspiration/hints\n",
    "        \n",
    "        t=t+1\n",
    "    \n",
    "    loss=loss[:t]\n",
    "    return w,loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fill in some reasonable values for a few optimization-related parameters that you may want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim={}\n",
    "optim['maxiter']=10000    # stop after at most this many iterations\n",
    "optim['lr']=.01           # (initial) learning rate\n",
    "optim['miniter']=50       # minimal number of iterations\n",
    "optim['mindelta']=1.0001  # tolerance for loss drop; stop if loss(t-m)/loss(t) < this value, for some m\n",
    "optim['lrdrop']=.5        # when dropping learning rate, use this factor to reduce it\n",
    "optim['dropfreq']=100     # how often do we drop the learning rate (every this many iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to train a linear model with the code below; you can compare it to the closed form solution to make sure you get similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define feature transform\n",
    "_,mu,sigma=polyFv(xtrain,(0,1))\n",
    "fmap=lambda x: polyFv(x,(0,1),mu,sigma)\n",
    "# train model\n",
    "w,loss = GD(xtrain,ytrain,fmap,optim,lossSquared)\n",
    "# evaluate on train and val sets\n",
    "lossTrain=lossSquared(fmap(xtrain),ytrain,w)\n",
    "lossVal=lossSquared(fmap(xval),yval,w)\n",
    "print('[%d iter] train RMSE %.4f, val RMSE %.4f'%(len(loss),sqrt(lossTrain),sqrt(lossVal)))\n",
    "# plot the loss value over iterations, to observe the learning process\n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can explore some more feature spaces..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsets=((0,),(0,1),(0,1,2),(0,1,2,3))  # try others!\n",
    "\n",
    "models={}\n",
    "for f in fsets:\n",
    "    models[f]={}\n",
    "    _,mu,sigma=polyFv(xtrain,f)\n",
    "    fmap=lambda x: polyFv(x,f,mu,sigma)\n",
    "    models[f]['w'],models[f]['loss'] = GD(xtrain,ytrain,fmap,optim,lossSquared)\n",
    "    models[f]['lossTrain']=lossSquared(fmap(xtrain),ytrain,models[f]['w'])\n",
    "    models[f]['lossVal']=lossSquared(fmap(xval),yval,models[f]['w'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in fsets:\n",
    "    print('Model %s: train RMSE %.4f, val RMSE %.4f' %(f,sqrt(models[f]['lossTrain']),sqrt(models[f]['lossVal'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the exploration above, at this point you will have selected a model (i.e., feature space in which a linear model operates). Now let's see what changes when we move to an asymmetric loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asymmLoss(X, y,w, alpha,getGrad=False):\n",
    "    \"\"\"\n",
    "    Get the asymmetric loss given data X, weight w and ground truth y\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2D array\n",
    "        N x d+1 design matrix (row per example)\n",
    "    y : 1D array\n",
    "        Observed function values\n",
    "    w : 1D array\n",
    "        d+1 length vector\n",
    "    Returns\n",
    "    -------\n",
    "    loss : a scalar\n",
    "        The loss calculated by equation in problem set 1\n",
    "    \"\"\"\n",
    "    \n",
    "    #### YOUR CODE HERE to implement asymmetric loss, which weighs positive negative errors (i.e., underestimated values) \n",
    "    # with alpha, and positive errors with 1.\n",
    "    # You should be able to cut and paste the code from squaredLoss, and only modify it a little.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for convenience, we will define this lambda expression so we could call it inside GD without passing alpha to GD (which does\n",
    "# not expect it -- we sort of make the asymmetric loss backward compatible...)\n",
    "def aloss(alpha):\n",
    "    return lambda x,y,w,getGrad: asymmLoss(x,y,w,alpha,getGrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train a model, say a linear one, using code very similar to what we did before (the only difference is in the use of\n",
    "the asymmetric loss, both in calling GD and in evaluating the resulting model on the val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.05\n",
    "\n",
    "_,mu,sigma=polyFv(xtrain,(0,1))\n",
    "fmap=lambda x: polyFv(x,(0,1),mu,sigma)\n",
    "w,loss = GD(xtrain,ytrain,fmap,optim,aloss(alpha))\n",
    "rmseTrain=lossSquared(fmap(xtrain),ytrain,w)\n",
    "rmseVal=lossSquared(fmap(xval),yval,w)\n",
    "alossVal=asymmLoss(fmap(xval),yval,w,alpha)\n",
    "print('[%d iter] train RMSE %.4f, val RMSE %.4f, weighted %.4f'%(len(loss),sqrt(lossTrain),sqrt(lossVal),sqrt(alossVal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can experiment, using the validation set to select the model class and set parameter values (similarly to what was done with symmetric loss).\n",
    "\n",
    "When we are done, we can evaluate the best model we have found on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE for evaluating different models with asymmetric loss, and selecting best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, we now have a model selected under regular squared loss, and a model selected under asymmetric loss. We can evaluate each of these two models on the test set, with its \"native\" loss and with the other loss (so you will get a total of four values: \"symmetric model with symmetric loss\", \"symmetric model with asymmetric loss\" etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE for computing those loss values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we conclude from this comparison?... Which model is \"better\", and in what sense?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
