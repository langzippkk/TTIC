{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTIC 31020 Introduction to Machine Learning: SVMs via subgradient descent\n",
    "---\n",
    "- The SVM learning problem is a convex optimization problem. It is presented either in its primal or dual form. The primal form is one of norm minimization subject to constraints, while the dual is a quadratic programming problem that is typically solved with an off-the-shelf QP solver. As a result, most popular machine learning libraries (e.g., LIBSVM underneath sklearn) solve the dual. In this assignment, you will solve the primal using subgradient descent, both the simple linear version and a version that supports nonlinear kernels. That is, we will solve the following optimization problem (we're not using the bias weight $b$ below):\n",
    "\\begin{equation*}\n",
    "\t\\mathbf{w}^* = \\arg\\!\\min \\frac{\\lambda}{2} ||\\mathbf{w}||^2 + \\frac{1}{n} \\sum_{(\\mathbf{x}, y) \\in S} [\\, 1 -  y(\\mathbf{w} \\cdot \\mathbf{x}) \\,]_+\n",
    "\\end{equation*}\n",
    "where $ S $ is a train set of $ \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n $. We use SGD with batch size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_decision_countour(svm, X, y, grid_size=100):\n",
    "    x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "    y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, grid_size),\n",
    "                         np.linspace(y_min, y_max, grid_size),\n",
    "                         indexing='ij')\n",
    "    data = np.stack([xx, yy], axis=2).reshape(-1, 2)\n",
    "    pred = svm.predict(data).reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, pred,\n",
    "                 cmap=cm.Paired,\n",
    "                 levels=[-0.001, 0.001],\n",
    "                 extend='both',\n",
    "                 alpha=0.8)\n",
    "    flatten = lambda m: np.array(m).reshape(-1,)\n",
    "    plt.scatter(flatten(X[:, 0]), flatten(X[:, 1]),\n",
    "                c=flatten(y), cmap=cm.Paired)\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.plot()\n",
    "\n",
    "\n",
    "def test_linear_SVM(svm, num_samples=500):\n",
    "    \"\"\"test linear svm\n",
    "    \"\"\"\n",
    "    X = npr.random((num_samples, 2)) * 2 - 1\n",
    "    y = 2 * (X.sum(axis=1) > 0) - 1.0\n",
    "\n",
    "    svm.fit(X, y)\n",
    "    plot_decision_countour(svm, X, y)\n",
    "\n",
    "\n",
    "def test_rbf_SVM(svm, num_samples=500):\n",
    "    \"\"\"test rbf svm\n",
    "    \"\"\"\n",
    "    X = npr.random((num_samples, 2)) * 2 - 1\n",
    "    y = 2 * ((X ** 2).sum(axis=1) - 0.5 > 0) - 1.0\n",
    "\n",
    "    svm.fit(X, y)\n",
    "    plot_decision_countour(svm, X, y)\n",
    "\n",
    "\n",
    "def compute_acc(model, X, y):\n",
    "    pred = model.predict(X)\n",
    "    size = len(y)\n",
    "    num_correct = (pred == y).sum()\n",
    "    acc = num_correct / float(size)\n",
    "    print(\"{} out of {} correct, acc {:.3f}\".format(num_correct, size, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearSVM():\n",
    "    def __init__(self, lamb=1, num_epochs=20):\n",
    "        \"\"\"initialize the svm\n",
    "        \n",
    "        Args:\n",
    "            lamb: the lambda parameter. Since lambda is a reserved keyword we use lamb\n",
    "            num_epochs: number of epochs for training. T = data_size * num_epochs\n",
    "        \"\"\"\n",
    "        self.w = None\n",
    "        self.lamb = lamb\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model on the data\n",
    "        \n",
    "        Args:\n",
    "            X: [N x d] data matrix\n",
    "            y: [N, ] array of labels\n",
    "        \n",
    "        Returns:\n",
    "            self, in case you want to build a pipeline\n",
    "        \"\"\"\n",
    "        assert np.ndim(X) == 2, 'data matrix X expected to be 2d'\n",
    "        assert np.ndim(y) == 1, 'labels expected to be 1d'\n",
    "        N, d = X.shape\n",
    "        assert N == y.shape[0], 'expect [N, d] data matrix and [N] labels'\n",
    "        self.w = np.zeros(shape=(d,))\n",
    "        T = int(self.num_epochs * N)\n",
    "        lamb = self.lamb\n",
    "        for t in range(1, T + 1):\n",
    "            inx = npr.choice(N)\n",
    "            lr = 1 / (lamb * t)\n",
    "            x, lbl = X[inx], y[inx]\n",
    "            margin = # YOUR CODE HERE for calculating labels multiplied by confidence margins (hint: you may want to use the predict function below)\n",
    "            if margin < 1:\n",
    "                self.w = # YOUR CODE HERE for defining the weight update when margin < 1\n",
    "            else:\n",
    "                self.w = # YOUR CODE HERE for defining the weight update when margin >= 1\n",
    "        print(\"fitting complete\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, binarize=True):\n",
    "        \"\"\"make a prediction and return either the confidence margin or label\n",
    "        \n",
    "        Args:\n",
    "            X: [N, d] array of data or [d,] single data point\n",
    "            binarize: if True, then return the label, else the confidence margin\n",
    "        \n",
    "        Returns:\n",
    "            Either confidence margin or predicted label\n",
    "        \"\"\"\n",
    "        if self.w is None:\n",
    "            raise ValueError(\"go fit the data first\")\n",
    "        X = np.atleast_2d(X)\n",
    "        assert X.shape[1] == self.w.shape[0]\n",
    "        res = # YOUR CODE HERE for computing confidence margins\n",
    "        res = res.squeeze()\n",
    "        if binarize:\n",
    "            return # YOUR CODE HERE for converting confidence margins to labels\n",
    "        else:\n",
    "            return res\n",
    "\n",
    "    def clone(self):\n",
    "        \"\"\"construct a fresh copy of myself\n",
    "        \"\"\"\n",
    "        return LinearSVM(self.lamb, self.num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm = LinearSVM(lamb=0.01, num_epochs=20)\n",
    "test_linear_SVM(svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Kernel(object):\n",
    "    \"\"\"A class containing a few kernels.\n",
    "    \n",
    "    \n",
    "    Note that for each Kernel, the input comes in the form of \n",
    "    Args:\n",
    "        X: [M, d]\n",
    "        Y: [N, d]\n",
    "    Returns:\n",
    "        a matrix of shape [M, N] filled with kernel outputs\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def linear():\n",
    "        def f(X, Y):\n",
    "            return X.dot(Y.T)\n",
    "        return f\n",
    "\n",
    "    @staticmethod\n",
    "    def gaussian(gamma):\n",
    "        def f(X, Y):\n",
    "            # insert a new axis so that we can vectorize (with broadcasting)\n",
    "            X = X[:, np.newaxis, :]\n",
    "            exponent = # YOUR CODE HERE\n",
    "            return np.exp(exponent)\n",
    "        return f\n",
    "\n",
    "    @staticmethod\n",
    "    def _poly(dimension, offset):\n",
    "        def f(X, Y):\n",
    "            return (offset + X.dot(Y.T)) ** dimension\n",
    "        return f\n",
    "\n",
    "    @staticmethod\n",
    "    def inhomogenous_polynomial(dimension):\n",
    "        return Kernel._poly(dimension=dimension, offset=1.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def homogenous_polynomial(dimension):\n",
    "        return Kernel._poly(dimension=dimension, offset=0.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def hyperbolic_tangent(kappa, c):\n",
    "        def f(X, Y):\n",
    "            return np.tanh(kappa * X.dot(Y.T) + c)\n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KernelSVM():\n",
    "    def __init__(self, kernel, lamb=1, num_epochs=20):\n",
    "        \"\"\"initialize the kernel svm\n",
    "        \n",
    "        Args:\n",
    "            kernel: a kernel function\n",
    "            lamb: the lambda parameter. Since lambda is a reserved keyword we use lamb\n",
    "            num_epochs: number of epochs for training. T = data_size * num_epochs\n",
    "        \"\"\"\n",
    "        self.num_epochs = num_epochs\n",
    "        self.lamb = lamb\n",
    "        assert callable(kernel), 'kernel should be a function'\n",
    "        self.kernel = kernel\n",
    "        self.repr_vec = None  # [K, d] of K (untransformed) representation vec\n",
    "        self.repr_vec_lbls = None  # [K,] for the labels of the repr vec\n",
    "        self.repr_vec_weights = None\n",
    "        self.t = 1  # current step index. used by predict\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model on the data\n",
    "        \n",
    "        Args:\n",
    "            X: [N x d] data matrix\n",
    "            y: [N, ] array of labels\n",
    "        \n",
    "        Returns:\n",
    "            self, in case you want to build a pipeline\n",
    "        \"\"\"\n",
    "        assert np.ndim(X) == 2, 'data matrix X expected to be 2d'\n",
    "        assert np.ndim(y) == 1, 'labels expected to be 1d'\n",
    "        N, d = X.shape\n",
    "        assert N == y.shape[0], 'expect [N, d] data matrix and [N] labels'\n",
    "        # initialize to dummies\n",
    "        self.repr_vec             = np.zeros(shape=(1, d))\n",
    "        self.repr_vec_lbls        = np.zeros(shape=(1,))\n",
    "        self.repr_vec_weights     = np.zeros(shape=(1,))\n",
    "\n",
    "        counts = np.zeros(shape=(N,))\n",
    "        T = int(self.num_epochs * N)\n",
    "        for t in range(1, T + 1):\n",
    "            self.t = t\n",
    "            inx = npr.choice(N)\n",
    "            x, lbl = X[inx], y[inx]\n",
    "            margin = # YOUR CODE HERE\n",
    "            if margin < 1:\n",
    "                # update the counts\n",
    "                # YOUR CODE HERE\n",
    "                \n",
    "                # update the representation vectors\n",
    "                self.repr_vec = # YOUR CODE HERE\n",
    "                self.repr_vec_weights = # YOUR CODE HERE\n",
    "                self.repr_vec_lbls = # YOUR CODE HERE\n",
    "        print(\"fitting complete\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, binarize=True):\n",
    "        \"\"\"make a prediction and return either the confidence margin or label\n",
    "        \n",
    "        Args:\n",
    "            X: [N, d] array of data or [d,] single data point\n",
    "            binarize: if True, then return the label, else the confidence margin\n",
    "        \n",
    "        Returns:\n",
    "            Either confidence margin or predicted label\n",
    "        \"\"\"\n",
    "        assert self.repr_vec is not None, 'go fit the data first'\n",
    "        X = np.atleast_2d(X)\n",
    "        if X.shape[0] > 50:\n",
    "            # we want to avoid vectorization when N is too large. Why?\n",
    "            # brave souls can try to remove this condition and see.\n",
    "            ker_output = np.hstack([\n",
    "                self.kernel(self.repr_vec, x.reshape(1, -1))\n",
    "                for x in X\n",
    "            ])\n",
    "        else:\n",
    "            ker_output = self.kernel(self.repr_vec, X)\n",
    "        res = # YOUR CODE HERE\n",
    "        res = res.squeeze()\n",
    "        if binarize:\n",
    "            # get labels from results\n",
    "            lbls = # YOUR CODE HERE\n",
    "            return lbls\n",
    "        else:\n",
    "            return res\n",
    "\n",
    "    def clone(self):\n",
    "        \"\"\"construct a fresh copy of myself\n",
    "        \"\"\"\n",
    "        return KernelSVM(self.kernel, self.lamb, self.num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm = KernelSVM(Kernel.gaussian(gamma=0.5), lamb=1e-4, num_epochs=20)\n",
    "test_rbf_SVM(svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that our SVM is working on the toy dataset, let's do sentiment analysis on [tweets on US airline service quality](https://www.kaggle.com/crowdflower/twitter-airline-sentiment/version/2). (WARNING: expletives unfiltered)\n",
    "---\n",
    "- As shown below, our data comes in the form of a csv table. The columns most relevant to our task are 'text' and 'airline_sentiment'.\n",
    "- Data must be represented as a [N x d] matrix, but what we have on our hands is unstructured text.\n",
    "- The simplest solution to transform an airline review into a vector is [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model). We maintain a global vocabulary of word patterns gathered from our corpus, with single words such as \"great\", \"horrible\", and optionally consecutive words (N-grams) like \"friendly service\", \"luggage lost\". Suppose we have already collected a total of 10000 such patterns, to transform a sentence into a 10000-dimensional vector, we simply scan it and look for the patterns that appear and set their corresponding entries to 1 and leave the rest at 0. What we end up with is a sparse vector that can be fed into SVMs.\n",
    "- For this exercise we use the basic text processing routines in nltk and sklearn\n",
    "- Our data is not balanced, with significant more negatives than neutral + positives. Therefore we group neutral and positive into one category and the final ratio of non-negative to negative is about 1:2. This is consistent across train, val, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import re, nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_root = 'pset3-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, val, test = \\\n",
    "    pd.read_csv(osp.join(data_root, 'train.csv')), \\\n",
    "    pd.read_csv(osp.join(data_root, 'val.csv')), \\\n",
    "    pd.read_csv(osp.join(data_root, 'devtest.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train.airline_sentiment.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check these out \n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def tokenize_normalize(tweet):\n",
    "    only_letters = re.sub(\"[^a-zA-Z]\", \" \", tweet)\n",
    "    tokens = nltk.word_tokenize(only_letters)[2:]\n",
    "    lower_case = [l.lower() for l in tokens]\n",
    "    filtered_result = list(filter(lambda l: l not in stop_words, lower_case))\n",
    "    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the sklearn vectorizer scans our corpus, build the vocabulary, and changes text into vectors\n",
    "vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode', \n",
    "    lowercase=True, \n",
    "    tokenizer=tokenize_normalize,\n",
    "    ngram_range=(1,1),  # you may want to try 2 grams. The vocab will get very large though\n",
    ")\n",
    "# first learn the vocabulary\n",
    "vectorizer.fit(pd.concat([train, val, test]).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take a peek at the vocabulary learnt. We have the terms and the counts\n",
    "print( list(vectorizer.vocabulary_.items())[:10] )\n",
    "print(\"\\n vocabulary size {}\".format(len(vectorizer.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = {}\n",
    "y = {}\n",
    "X['train'] = vectorizer.transform(train.text).toarray()\n",
    "X['val'] = vectorizer.transform(val.text).toarray()\n",
    "X['test'] = vectorizer.transform(test.text).toarray()\n",
    "\n",
    "# note that our data is 10250 dimensional. \n",
    "# This is a little daunting for laptops and coming up with a manageable vector representation is a major topic in Natural Language Processing.\n",
    "print(X['train'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert the word labels of 'positive', 'neutral', 'negative' into integer labels\n",
    "# note that positive and neural belong to one category, labeled as 1, while negative stands alone as the other\n",
    "for name, dataframe in zip(['train', 'val', 'test'], [train, val, test]):\n",
    "    sentiments_in_words = dataframe['airline_sentiment'].tolist()\n",
    "    int_lbls = np.array( list(map(lambda x: -1 if x == 'negative' else 1, sentiments_in_words)), dtype=np.int32 )\n",
    "    y[name] = int_lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm = LinearSVM(lamb=1e-2, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm.fit(X['train'], y['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compute_acc(svm, X['train'], y['train'])\n",
    "compute_acc(svm, X['val'], y['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try to come up with a better text feature representation. We threw out all the emojis. >< what a waste\n",
    "- Given the high feature dimensionality of our primitive text processing, we do not recommend using kernel SVM here. It could take a long time to train. If you reduce the feature dimensionality, then it's a different story\n",
    "- Play around with different parameter settings and find the best setting on the validation set, then evaluate on the devtest set when you're finished tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compute_acc(svm, X['test'], y['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
